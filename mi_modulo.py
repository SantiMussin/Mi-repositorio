# -*- coding: utf-8 -*-
"""mi_modulo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WnOlVVQCx9LkC03Ssr6SITaChblnLWkw
"""

from scipy.stats import expon
from scipy.stats import t
from scipy.stats import chi2
from numpy.random import randint
import matplotlib.pyplot as plt
import numpy as np
import statsmodels as sm
from scipy.stats import norm
import pandas as pd
import random
import statsmodels.api as sm
from sklearn.metrics import auc
from statsmodels.stats.anova import anova_lm

class ResumenNumerico:
  def __init__(self,datos):
    self.datos=datos

  def calculo_de_media(self):
    media=np.mean(self.datos)
    return media

  def calculo_de_mediana(self):
    mediana=np.median(self.datos)
    return mediana

  def calculo_de_desvio_estandar(self):
    desvio=np.std(self.datos)
    return desvio

  def calculo_de_cuartiles(self):
    q1=np.percentile(self.datos,25)
    q2=np.percentile(self.datos,50)
    q3=np.percentile(self.datos,75)
    return [q1, q2, q3]

  def resumen_numerico(self):
    res_num = {
    'Media': self.calculo_de_media(self.datos),
    'Mediana': self.calculo_de_mediana(self.datos),
    'Desvio': self.calculo_de_desvio_estandar(self.datos),
    'Cuartiles': self.calculo_de_cuartiles(self.datos),
    'Mínimo': min(self.datos),
    'Máximo': max(self.datos)
    }
    return res_num


class ResumenGrafico:
  def __init__(self,datos):
    self.datos = np.array(datos)

  def miqqplot(self):
    data_ord = np.sort(self.datos)
    desvio = np.std(data_ord)
    media = np.mean(data_ord)
    x_ord_s = (data_ord-media)/desvio
    cuantiles_teoricos = []
    for i in range(1,len(self.datos)+1):
      cuantiles_teoricos.append(norm.ppf(i/(len(self.datos+1))))

    plt.scatter(cuantiles_teoricos, x_ord_s, color='blue', marker='o')
    plt.xlabel('Cuantiles teóricos')
    plt.ylabel('Cuantiles muestrales')
    plt.plot(cuantiles_teoricos,cuantiles_teoricos , linestyle='-', color='red')
    plt.show()

    return cuantiles_teoricos

  def evaluacion_histograma(self, h, X):
    maximo = np.max(X)
    minimo = np.min(X)
    vectores = np.arange(minimo,maximo+3*h,h)
    datos_en_rango = []

    for i in range(len(vectores)-1):
      cantidad_datos = 0
      for x in self.datos:
        if x>vectores[i] and x<=vectores[i+1]:
          cantidad_datos = cantidad_datos+1
      datos_en_rango = datos_en_rango+[cantidad_datos]

    datos_en_rango = np.array(datos_en_rango)
    densidad_en_rango = ((datos_en_rango)/len(self.datos))/h

    evaluacion = []

    for i in range(len(vectores)-1):
      for j in X:
        if j>vectores[i] and j<=vectores[i+1]:
         evaluacion=evaluacion+[densidad_en_rango[i]]

    if minimo<=vectores[0]:
      evaluacion.append(densidad_en_rango[0])

    return evaluacion

  def kernel_gaussiano(self,u):
    valor_kernel_gaussiano = 1/(np.sqrt(2*np.pi)) * (np.e **(-0.5 * (u**2)))
    return valor_kernel_gaussiano

  def mi_densidad(self,values,h,kernel):
    density = []
    if kernel == "uniforme":
      for x in values:
        cant_adentro = 0
        for Xi in self.datos:
          if -1/2 < (Xi - x)/h <= 1/2:
            cant_adentro += 1
        density.append(cant_adentro/(len(self.datos)*h))

    if kernel == "gaussiano":
      for x in values:
        cant_adentro = 0
        for Xi in self.datos:
          u = (Xi - x) / h
          gauss = self.kernel_gaussiano(u)
          cant_adentro += gauss
        density.append(cant_adentro/(len(self.datos)*h))
    return density

class GeneradoraDeDatos:
  def __init__(self,ndatos):
    self.n=ndatos

  def generar_datos_dist_norm(self,media,varianza):
    datos_normal=np.random.normal(media,varianza,self.n)
    return datos_normal

  def generar_datos_exponenciales(self,n):
    datos_exponenciales=expon.rvs(0,1,size=n)
    return datos_exponenciales

  def generar_datos_BS(self):
    u = np.random.uniform(size=(self.n,))
    y = u.copy()
    ind = np.where(u > 0.5)[0]
    y[ind] = np.random.normal(0, 1, size=len(ind))
    for j in range(5):
        ind = np.where((u > j * 0.1) & (u <= (j+1) * 0.1))[0]
        y[ind] = np.random.normal(j/2 - 1, 1/10, size=len(ind))
    return y

  def generar_datos_Tstudent(self,n,grados_libertad):
    datos=t.rvs(df=grados_libertad, size=n)
    return datos

  def generar_datos_uniforme(self,n):
    datos_uniforme=np.random.uniform(0,1,n)
    return datos_uniforme

  def teorica_normal(self,grilla,media,varianza):
    densidad_teorica=norm.pdf(grilla,media,varianza)
    return densidad_teorica

  def teorica_BS(self,datos):
    sum=0
    for x in range(5):
      a=norm.pdf(datos,(x/2)-1,1/10)
      sum+=a

    densidad=0.5*norm.pdf(datos,0,1)+sum*0.1
    return densidad

  def teorica_exponencial(self,grilla):
    teorica=expon.pdf(grilla)
    return teorica

class Regresion:
  def __init__(self,x,y):
    self.x=x
    self.y=y

  def predecir(self,x_new,modelo):
    X_new=[1, x_new]
    prediccion = modelo.predict(X_new)
    return prediccion

  def almacenar_datos(self,resultado):
    betas = resultado.params
    std_errores = resultado.bse
    t_obs = resultado.tvalues
    p_valores = resultado.pvalues
    #DataFrame para almacenar los resultados
    informe = pd.DataFrame({
        'Beta': betas,
        'Std Error': std_errores,
        't_obs': t_obs,
        'p_value': p_valores
    })
    return informe

  def entrenar_modelo(self,datos,porcentaje_test,semilla):
    random.seed(semilla)
    cuales = random.sample(range(len(datos)), round((porcentaje_test*(len(datos)))/100))
    datos_train = datos.drop(cuales)
    datos_test = datos.iloc[cuales]
    return datos_train,datos_test

class RegresionLineal(Regresion):
  def __init__(self, x, y):
        super().__init__(x, y)

  def ajustar_modelo(self):
    X = sm.add_constant(self.x)
    modelo = sm.OLS(self.y, X)
    self.modelo_ajustado = modelo.fit()
    return self.modelo_ajustado

  def correlacion(self):
    correlacion=np.corrcoef(self.x,self.y)
    return correlacion

  def grafica(self,recta):
    plt.scatter(self.x, self.y, marker="o", facecolors="none", edgecolors="blue")
    plt.plot(self.x,recta)
    plt.xlabel("Predictora")
    plt.ylabel("Respuesta")
    plt.show()

  def supuestos(self):
    plt.figure(figsize=(10,6))
    plt.scatter(self.modelo_ajustado.fittedvalues, self.modelo_ajustado.resid)
    plt.xlabel('Valores ajustados')
    plt.ylabel('Residuos')
    plt.title('Valores ajustados vs Residuos')
    plt.axhline(0, color='red', linestyle='--', linewidth=2)
    plt.figure(2)
    inicio_ResumenGrafico=ResumenGrafico(self.modelo_ajustado.resid)
    qqplot = inicio_ResumenGrafico.miqqplot()
    plt.show()

  def intervalos(self,x_new,alpha,modelo):
    X_new = sm.add_constant(x_new)
    prediccion = modelo.predict(X_new)
    print('intervalo de confianza para el valor esperado de  Y  para un valor particular de  X:',prediccion.conf_int(alpha =alpha))
    print('intervalo de predicción de  Y  para un valor particular de  X:',prediccion.conf_int(obs=True , alpha =alpha))

  def Anova(m,M):
    anova_resultado = anova_lm(m, M)
    print(anova_resultado)
    return anova_resultado

  def R_cuadrado(self):
    R_cuadrado=self.modelo_ajustado.rsquared
    R_cuadrado_ajustado = self.modelo_ajustado.rsquared_adj
    return R_cuadrado,R_cuadrado_ajustado

class RegresionLogistica(Regresion):
  def __init__(self, x, y):
    super().__init__(x, y)

  def ajustar_modelo(self):
    X = sm.add_constant(self.x)
    modelo = sm.Logit(self.y, X)
    self.resultado = modelo.fit()
    return self.resultado

  def prediccion_con_umbral(self,umbral,x_test,resultado):
    X_test = sm.add_constant(x_test)
    prediccion = resultado.predict(X_test)
    y_pred = 1*(prediccion >= umbral)
    return y_pred

  def matriz_de_confusion(self,y_pred,y_test):
    a=np.sum((y_pred==1)&(y_test==1))
    b=np.sum((y_pred==1)&(y_test==0))
    c=np.sum((y_pred==0)&(y_test==1))
    d=np.sum((y_pred==0)&(y_test==0))
    matriz_confusion=np.array([[a,b],[c,d]])
    error_total_mala_clasificacion=(c+b)/(a+b+c+d)

    especificidad=d/(d+b)
    sensibilidad=a/(a+c)

    return matriz_confusion,error_total_mala_clasificacion,especificidad,sensibilidad

  def curva_roc(self,y_test,x_test):
    grilla_p = np.linspace(0, 1, 100)
    sensibilidad = []
    especificidad = []
    for p in grilla_p:
      y_pred=self.prediccion_con_umbral(p,x_test,self.resultado)
      sensibilidad.append(sum((y_test==1)&(y_pred==1))/(sum((y_pred==0)&(y_test==1))+sum((y_test==1)&(y_pred==1))))
      especificidad.append(sum((y_test==0)&(y_pred==0))/(sum((y_pred==0)&(y_test==0))+sum((y_test==0)&(y_pred==1))))

    especificidad=np.array(especificidad)
    especificidad1=(-1)*especificidad+1

    plt.figure(1)
    plt.plot(grilla_p,sensibilidad)
    plt.plot(grilla_p,especificidad)
    plt.figure(2)
    plt.plot(especificidad1,sensibilidad)
    plt.xlabel('1-Especificidad')
    plt.ylabel('sensibilidad')
    plt.show()

    sensibilidad=np.array(sensibilidad)
    P_optimo=max(sensibilidad+(especificidad-1))

    roc_auc = auc(1-np.array(especificidad), sensibilidad)
    print('roc_auc: ', roc_auc)
    if 0.90 <= roc_auc <= 1:
      print('roc_auc clasificado como excelente')
    elif 0.80 <= roc_auc < 0.90 :
      print('roc_auc clasificado como bueno')
    elif 0.70 <= roc_auc <= 0.80:
      print('roc_auc clasificado como regular')
    elif 0.60 <= roc_auc <= 0.70:
      print('roc_auc clasificado como pobre')
    else:
      print('roc_auc clasificado como fallido')

    return P_optimo,roc_auc

class Dados:
  def __init__(self,tita,alfa,muestra):
    self.tita=tita
    self.alfa=alfa
    self.muestra=np.array(muestra)

  def boostrap(self,n):
    N=len(self.muestra)
    indices=[]
    for x in range(n):
      ind=randint(0,N)
      indices=indices+[ind]
    return self.muestra[indices]

  def intervalo_boostrap(self,N,n):
    titas_boot=[]
    z_critico=norm.ppf(q=1-self.alfa/2)

    for x in range(0,N):
      muestra=self.boostrap(n)
      tita_boot=(sum(muestra%2==0))/len(muestra)
      titas_boot.append(tita_boot)

    desvio_titas=np.std(titas_boot)
    intervalo=[self.tita-z_critico*desvio_titas,self.tita+z_critico*desvio_titas]
    plt.hist(titas_boot,density=True);
    return intervalo,desvio_titas

  def intervalo(self):
    z_critico=norm.ppf(q=1-self.alfa/2)
    desvio=np.sqrt((self.tita*(1-self.tita))/len(self.muestra))
    intervalo=[self.tita-z_critico*desvio,self.tita+z_critico*desvio]
    return intervalo

  def hipotesis_frecuencias(self,probabilidad,frecuencias):
    n=len(probabilidad)-1
    X_obs=sum(((frecuencias-probabilidad)**2)/probabilidad)
    chi_percentil=chi2.ppf(1-self.alfa,n)
    p_valor=1-chi2.cdf(X_obs,n)

    if X_obs > chi_percentil:
      print('el valor observado del estadístico es mayor al percentil de la distribución')
      print('la probabilidad de haber encontrado ese valor o uno más grande:',p_valor)
      print("Rechazamos la hipótesis nula")

    elif X_obs <= chi_percentil:
      print('el valor observado del estadístico es menor o igual al percentil de la distribución')
      print('la probabilidad de haber encontrado ese valor o uno más grande:',p_valor)
      print("No rechazamos la hipótesis nula")

    return p_valor,X_obs,chi_percentil

#from google.colab import drive
#import pandas as pd

# Lee el archivo CSV
#drive.mount('/content/drive')
#datos= pd.read_csv("/content/drive/MyDrive/ciencia de datos 3/salary.csv")

## Corré este código para ver características de la basd de datos
#print(datos.shape)
#nombres_columnas = datos.columns #muestra los nombres de las variables
#print(nombres_columnas)